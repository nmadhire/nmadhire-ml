{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":71485,"databundleVersionId":8059942,"sourceType":"competition"},{"sourceId":176951916,"sourceType":"kernelVersion"}],"dockerImageVersionId":30699,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"MODEL_PATH = '/kaggle/input/learning-agency-lab-nmadhire/learning_agency_lab/v1/checkpoint-800/'","metadata":{"execution":{"iopub.status.busy":"2024-05-11T12:24:06.548001Z","iopub.execute_input":"2024-05-11T12:24:06.548893Z","iopub.status.idle":"2024-05-11T12:24:06.552972Z","shell.execute_reply.started":"2024-05-11T12:24:06.548858Z","shell.execute_reply":"2024-05-11T12:24:06.552049Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"# put all imports\nimport pandas as pd\nfrom datasets import Dataset\nimport re\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\nimport torch\nfrom sklearn.metrics import cohen_kappa_score\nfrom transformers import AutoModelForSequenceClassification, TrainingArguments, Trainer, AutoTokenizer","metadata":{"execution":{"iopub.status.busy":"2024-05-11T12:24:06.554731Z","iopub.execute_input":"2024-05-11T12:24:06.555104Z","iopub.status.idle":"2024-05-11T12:24:06.565508Z","shell.execute_reply.started":"2024-05-11T12:24:06.555071Z","shell.execute_reply":"2024-05-11T12:24:06.564525Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"tokenizer_inference = AutoTokenizer.from_pretrained(MODEL_PATH)\nmodel = AutoModelForSequenceClassification.from_pretrained(MODEL_PATH)","metadata":{"execution":{"iopub.status.busy":"2024-05-11T12:24:06.567002Z","iopub.execute_input":"2024-05-11T12:24:06.567260Z","iopub.status.idle":"2024-05-11T12:24:07.695999Z","shell.execute_reply.started":"2024-05-11T12:24:06.567238Z","shell.execute_reply":"2024-05-11T12:24:07.695081Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"code","source":"# Tokenize the input data\ndef preprocess_function_inference(input):\n    return tokenizer_inference(input[\"full_text\"], \n                     truncation=True, \n                     max_length=512\n                    )","metadata":{"execution":{"iopub.status.busy":"2024-05-11T12:24:07.697044Z","iopub.execute_input":"2024-05-11T12:24:07.697331Z","iopub.status.idle":"2024-05-11T12:24:07.701854Z","shell.execute_reply.started":"2024-05-11T12:24:07.697308Z","shell.execute_reply":"2024-05-11T12:24:07.700963Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"code","source":"# create regex compiler\ndef create_re():\n    # Dictionary of English Contractions\n    contractions_dict = { \"ain't\": \"are not\",\"'s\":\" is\",\"aren't\": \"are not\",\n                     \"can't\": \"cannot\",\"can't've\": \"cannot have\",\n                     \"'cause\": \"because\",\"could've\": \"could have\",\"couldn't\": \"could not\",\n                     \"couldn't've\": \"could not have\", \"didn't\": \"did not\",\"doesn't\": \"does not\",\n                     \"don't\": \"do not\",\"hadn't\": \"had not\",\"hadn't've\": \"had not have\",\n                     \"hasn't\": \"has not\",\"haven't\": \"have not\",\"he'd\": \"he would\",\n                     \"he'd've\": \"he would have\",\"he'll\": \"he will\", \"he'll've\": \"he will have\",\n                     \"how'd\": \"how did\",\"how'd'y\": \"how do you\",\"how'll\": \"how will\",\n                     \"I'd\": \"I would\", \"I'd've\": \"I would have\",\"I'll\": \"I will\",\n                     \"I'll've\": \"I will have\",\"I'm\": \"I am\",\"I've\": \"I have\", \"isn't\": \"is not\",\n                     \"it'd\": \"it would\",\"it'd've\": \"it would have\",\"it'll\": \"it will\",\n                     \"it'll've\": \"it will have\", \"let's\": \"let us\",\"ma'am\": \"madam\",\n                     \"mayn't\": \"may not\",\"might've\": \"might have\",\"mightn't\": \"might not\", \n                     \"mightn't've\": \"might not have\",\"must've\": \"must have\",\"mustn't\": \"must not\",\n                     \"mustn't've\": \"must not have\", \"needn't\": \"need not\",\n                     \"needn't've\": \"need not have\",\"o'clock\": \"of the clock\",\"oughtn't\": \"ought not\",\n                     \"oughtn't've\": \"ought not have\",\"shan't\": \"shall not\",\"sha'n't\": \"shall not\",\n                     \"shan't've\": \"shall not have\",\"she'd\": \"she would\",\"she'd've\": \"she would have\",\n                     \"she'll\": \"she will\", \"she'll've\": \"she will have\",\"should've\": \"should have\",\n                     \"shouldn't\": \"should not\", \"shouldn't've\": \"should not have\",\"so've\": \"so have\",\n                     \"that'd\": \"that would\",\"that'd've\": \"that would have\", \"there'd\": \"there would\",\n                     \"there'd've\": \"there would have\", \"they'd\": \"they would\",\n                     \"they'd've\": \"they would have\",\"they'll\": \"they will\",\n                     \"they'll've\": \"they will have\", \"they're\": \"they are\",\"they've\": \"they have\",\n                     \"to've\": \"to have\",\"wasn't\": \"was not\",\"we'd\": \"we would\",\n                     \"we'd've\": \"we would have\",\"we'll\": \"we will\",\"we'll've\": \"we will have\",\n                     \"we're\": \"we are\",\"we've\": \"we have\", \"weren't\": \"were not\",\"what'll\": \"what will\",\n                     \"what'll've\": \"what will have\",\"what're\": \"what are\", \"what've\": \"what have\",\n                     \"when've\": \"when have\",\"where'd\": \"where did\", \"where've\": \"where have\",\n                     \"who'll\": \"who will\",\"who'll've\": \"who will have\",\"who've\": \"who have\",\n                     \"why've\": \"why have\",\"will've\": \"will have\",\"won't\": \"will not\",\n                     \"won't've\": \"will not have\", \"would've\": \"would have\",\"wouldn't\": \"would not\",\n                     \"wouldn't've\": \"would not have\",\"y'all\": \"you all\", \"y'all'd\": \"you all would\",\n                     \"y'all'd've\": \"you all would have\",\"y'all're\": \"you all are\",\n                     \"y'all've\": \"you all have\", \"you'd\": \"you would\",\"you'd've\": \"you would have\",\n                     \"you'll\": \"you will\",\"you'll've\": \"you will have\", \"you're\": \"you are\",\n                     \"you've\": \"you have\"}\n\n    # Regular expression for finding contractions\n    return re.compile('(%s)' % '|'.join(contractions_dict.keys()))\n\n# Function for expanding contractions\ndef expand_contractions(text):\n    # Dictionary of English Contractions\n    contractions_dict = { \"ain't\": \"are not\",\"'s\":\" is\",\"aren't\": \"are not\",\n                     \"can't\": \"cannot\",\"can't've\": \"cannot have\",\n                     \"'cause\": \"because\",\"could've\": \"could have\",\"couldn't\": \"could not\",\n                     \"couldn't've\": \"could not have\", \"didn't\": \"did not\",\"doesn't\": \"does not\",\n                     \"don't\": \"do not\",\"hadn't\": \"had not\",\"hadn't've\": \"had not have\",\n                     \"hasn't\": \"has not\",\"haven't\": \"have not\",\"he'd\": \"he would\",\n                     \"he'd've\": \"he would have\",\"he'll\": \"he will\", \"he'll've\": \"he will have\",\n                     \"how'd\": \"how did\",\"how'd'y\": \"how do you\",\"how'll\": \"how will\",\n                     \"I'd\": \"I would\", \"I'd've\": \"I would have\",\"I'll\": \"I will\",\n                     \"I'll've\": \"I will have\",\"I'm\": \"I am\",\"I've\": \"I have\", \"isn't\": \"is not\",\n                     \"it'd\": \"it would\",\"it'd've\": \"it would have\",\"it'll\": \"it will\",\n                     \"it'll've\": \"it will have\", \"let's\": \"let us\",\"ma'am\": \"madam\",\n                     \"mayn't\": \"may not\",\"might've\": \"might have\",\"mightn't\": \"might not\", \n                     \"mightn't've\": \"might not have\",\"must've\": \"must have\",\"mustn't\": \"must not\",\n                     \"mustn't've\": \"must not have\", \"needn't\": \"need not\",\n                     \"needn't've\": \"need not have\",\"o'clock\": \"of the clock\",\"oughtn't\": \"ought not\",\n                     \"oughtn't've\": \"ought not have\",\"shan't\": \"shall not\",\"sha'n't\": \"shall not\",\n                     \"shan't've\": \"shall not have\",\"she'd\": \"she would\",\"she'd've\": \"she would have\",\n                     \"she'll\": \"she will\", \"she'll've\": \"she will have\",\"should've\": \"should have\",\n                     \"shouldn't\": \"should not\", \"shouldn't've\": \"should not have\",\"so've\": \"so have\",\n                     \"that'd\": \"that would\",\"that'd've\": \"that would have\", \"there'd\": \"there would\",\n                     \"there'd've\": \"there would have\", \"they'd\": \"they would\",\n                     \"they'd've\": \"they would have\",\"they'll\": \"they will\",\n                     \"they'll've\": \"they will have\", \"they're\": \"they are\",\"they've\": \"they have\",\n                     \"to've\": \"to have\",\"wasn't\": \"was not\",\"we'd\": \"we would\",\n                     \"we'd've\": \"we would have\",\"we'll\": \"we will\",\"we'll've\": \"we will have\",\n                     \"we're\": \"we are\",\"we've\": \"we have\", \"weren't\": \"were not\",\"what'll\": \"what will\",\n                     \"what'll've\": \"what will have\",\"what're\": \"what are\", \"what've\": \"what have\",\n                     \"when've\": \"when have\",\"where'd\": \"where did\", \"where've\": \"where have\",\n                     \"who'll\": \"who will\",\"who'll've\": \"who will have\",\"who've\": \"who have\",\n                     \"why've\": \"why have\",\"will've\": \"will have\",\"won't\": \"will not\",\n                     \"won't've\": \"will not have\", \"would've\": \"would have\",\"wouldn't\": \"would not\",\n                     \"wouldn't've\": \"would not have\",\"y'all\": \"you all\", \"y'all'd\": \"you all would\",\n                     \"y'all'd've\": \"you all would have\",\"y'all're\": \"you all are\",\n                     \"y'all've\": \"you all have\", \"you'd\": \"you would\",\"you'd've\": \"you would have\",\n                     \"you'll\": \"you will\",\"you'll've\": \"you will have\", \"you're\": \"you are\",\n                     \"you've\": \"you have\"}\n    def replace(match):\n        return contractions_dict[match.group(0)]\n    return create_re().sub(replace, text)\n\ndef perform_eda(df):\n    df.rename(columns={\"score\": \"label\"}, inplace=True)\n    # Check for missing values in the input dataset\n    missing_values = df.isnull().sum()\n    print(missing_values)\n    \n    # Expanding Contractions in the reviews\n    df['full_text']=df['full_text'].apply(lambda essay_text:expand_contractions(essay_text))\n    \n    # keep lowercase words\n    df['full_text']=df['full_text'].apply(lambda x: x.lower())\n    df.drop(['essay_id'], axis=1, inplace=True)\n    \n    if 'label' in df:\n        df['label'] = df['label'].map(lambda x: x-1)\n    return Dataset.from_pandas(df)","metadata":{"execution":{"iopub.status.busy":"2024-05-11T12:24:07.704052Z","iopub.execute_input":"2024-05-11T12:24:07.704316Z","iopub.status.idle":"2024-05-11T12:24:07.730664Z","shell.execute_reply.started":"2024-05-11T12:24:07.704288Z","shell.execute_reply":"2024-05-11T12:24:07.729717Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"code","source":"test_df = pd.read_csv('/kaggle/input/learning-agency-lab-automated-essay-scoring-2/test.csv')\nprint(test_df)\ntest_dataset = perform_eda(test_df)\n\ntest_tokenized_dataset = test_dataset.map(preprocess_function_inference, batched=True).remove_columns(['full_text'])","metadata":{"execution":{"iopub.status.busy":"2024-05-11T12:24:07.731731Z","iopub.execute_input":"2024-05-11T12:24:07.732105Z","iopub.status.idle":"2024-05-11T12:24:07.801758Z","shell.execute_reply.started":"2024-05-11T12:24:07.732074Z","shell.execute_reply":"2024-05-11T12:24:07.800947Z"},"trusted":true},"execution_count":20,"outputs":[{"name":"stdout","text":"  essay_id                                          full_text\n0  000d118  Many people have car where they live. The thin...\n1  000fe60  I am a scientist at NASA that is discussing th...\n2  001ab80  People always wish they had the same technolog...\nessay_id     0\nfull_text    0\ndtype: int64\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/3 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ab79aa1e39a6488599d93817e384c93a"}},"metadata":{}}]},{"cell_type":"code","source":"from transformers import DataCollatorWithPadding\n\ndata_collator = DataCollatorWithPadding(tokenizer=tokenizer_inference)","metadata":{"execution":{"iopub.status.busy":"2024-05-11T12:24:07.802846Z","iopub.execute_input":"2024-05-11T12:24:07.803123Z","iopub.status.idle":"2024-05-11T12:24:07.807293Z","shell.execute_reply.started":"2024-05-11T12:24:07.803101Z","shell.execute_reply":"2024-05-11T12:24:07.806361Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"code","source":"#Using the same training arguments as training. Need to research more on this for inference\ntraining_args = TrainingArguments(\n    output_dir='/kaggle/working/learning_agency_lab/v1', \n    fp16=True,\n    learning_rate=2e-5,\n    num_train_epochs=1,\n    per_device_train_batch_size=4,\n    per_device_eval_batch_size=2,\n    gradient_accumulation_steps=4,\n    report_to=\"none\",\n    push_to_hub=False,\n    evaluation_strategy=\"steps\",\n    do_eval=True,\n    eval_steps=100,\n    save_total_limit=1,\n    save_strategy=\"steps\",\n    save_steps=100,\n    logging_steps=100,\n    lr_scheduler_type='linear',\n    metric_for_best_model=\"qwk\",\n    greater_is_better=True,\n    warmup_ratio=0.1,\n    weight_decay=0.01,\n    save_safetensors=True\n)","metadata":{"execution":{"iopub.status.busy":"2024-05-11T12:24:07.808532Z","iopub.execute_input":"2024-05-11T12:24:07.808904Z","iopub.status.idle":"2024-05-11T12:24:07.836759Z","shell.execute_reply.started":"2024-05-11T12:24:07.808880Z","shell.execute_reply":"2024-05-11T12:24:07.835957Z"},"trusted":true},"execution_count":22,"outputs":[]},{"cell_type":"code","source":"trainer = Trainer( \n        model=model,\n        args=training_args,\n        train_dataset=test_tokenized_dataset,\n        data_collator=data_collator,\n        tokenizer=tokenizer_inference,\n    )","metadata":{"execution":{"iopub.status.busy":"2024-05-11T12:24:07.837837Z","iopub.execute_input":"2024-05-11T12:24:07.838170Z","iopub.status.idle":"2024-05-11T12:24:08.359063Z","shell.execute_reply.started":"2024-05-11T12:24:07.838141Z","shell.execute_reply":"2024-05-11T12:24:08.358045Z"},"trusted":true},"execution_count":23,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/accelerate/accelerator.py:436: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: \ndataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)\n  warnings.warn(\n","output_type":"stream"}]},{"cell_type":"code","source":"# SAVE PREDICTIONS\npredictions = trainer.predict(test_tokenized_dataset).predictions","metadata":{"execution":{"iopub.status.busy":"2024-05-11T12:24:08.362447Z","iopub.execute_input":"2024-05-11T12:24:08.362803Z","iopub.status.idle":"2024-05-11T12:24:08.702930Z","shell.execute_reply.started":"2024-05-11T12:24:08.362773Z","shell.execute_reply":"2024-05-11T12:24:08.702140Z"},"trusted":true},"execution_count":24,"outputs":[{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}}]},{"cell_type":"code","source":"submission_path = '/kaggle/input/learning-agency-lab-automated-essay-scoring-2/sample_submission.csv'\n\nprint(submission_path)","metadata":{"execution":{"iopub.status.busy":"2024-05-11T12:24:08.704031Z","iopub.execute_input":"2024-05-11T12:24:08.704272Z","iopub.status.idle":"2024-05-11T12:24:08.708841Z","shell.execute_reply.started":"2024-05-11T12:24:08.704250Z","shell.execute_reply":"2024-05-11T12:24:08.707959Z"},"trusted":true},"execution_count":25,"outputs":[{"name":"stdout","text":"/kaggle/input/learning-agency-lab-automated-essay-scoring-2/sample_submission.csv\n","output_type":"stream"}]},{"cell_type":"code","source":"submission_df = pd.read_csv(submission_path)\nsubmission_df[\"score\"] = predictions.argmax(axis=1)+1","metadata":{"execution":{"iopub.status.busy":"2024-05-11T12:24:08.709971Z","iopub.execute_input":"2024-05-11T12:24:08.710282Z","iopub.status.idle":"2024-05-11T12:24:08.721342Z","shell.execute_reply.started":"2024-05-11T12:24:08.710257Z","shell.execute_reply":"2024-05-11T12:24:08.720335Z"},"trusted":true},"execution_count":26,"outputs":[]},{"cell_type":"code","source":"submission_df.score = submission_df.score.astype('int32')\nsubmission_df.to_csv('submission.csv',index=False)","metadata":{"execution":{"iopub.status.busy":"2024-05-11T12:24:08.722491Z","iopub.execute_input":"2024-05-11T12:24:08.722808Z","iopub.status.idle":"2024-05-11T12:24:08.729262Z","shell.execute_reply.started":"2024-05-11T12:24:08.722783Z","shell.execute_reply":"2024-05-11T12:24:08.728401Z"},"trusted":true},"execution_count":27,"outputs":[]}]}